name: "LtoAB"
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {    phase: TRAIN  }
  transform_param {
    # mirror: true
    # crop_size: 176
    mirror: 0  # 1 = on, 0 = off
    crop_size: 256
  }
  data_param {
    # source: "/PATH/TO/DATA/ilsvrc_lmdb/caffe-train-lmdb"
    source: "/data/big/rzhang/dataset/ilsvrc_lmdb/caffe-train-lmdb"
    # batch_size: 40
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  include {    phase: TEST  }
  transform_param {
   # mirror: true
   # crop_size: 176
    mirror: 0  # 1 = on, 0 = off
    crop_size: 256
  }
  data_param {
    # source: "/PATH/TO/DATA/ilsvrc_lmdb/caffe-val-lmdb"
    source: "/data/big/rzhang/dataset/ilsvrc_lmdb/caffe-val-lmdb"
    # batch_size: 40
    batch_size: 10
    backend: LMDB
  }
}
layer { # compute gray mask
  type: 'Python'
  name: 'nongray_mask'
  bottom: "data"
  top: "img_lab" # quantized gt colors
  python_param {
    module: 'caffe_traininglayers'
    layer: 'BGR2LabLayer'
  }
}

# ****************************
# ***** Color Conversion *****
# ****************************
# layer { # Convert to lab
#   name: "img_lab"
#   type: "ColorConv"
#   bottom: "img"
#   top: "img_lab"
#   propagate_down: false
#   color_conv_param {
#     input: 0 # BGR
#     output: 3 # Lab
#   }
# }
layer {
  name: "img_slice"
  type: "Slice"
  bottom: "img_lab"
  top: "img_l" # [0,100]
  top: "data_ab" # [-110,110]
  propagate_down: false
  slice_param {
    axis: 1
    slice_point: 1
  }
}
layer { # 0-center data_l channel
  name: "data_l_meansub"
  type: "Scale"
  bottom: "img_l"
  top: "data_l" # [-50,50]
  propagate_down: false
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  scale_param {
    bias_term: True
    filler {      type: 'constant'      value: 1    }
    bias_filler {      type: 'constant'      value: -50    }
  }
}
# ****************************
# ***** PROCESS LABELS *******
# ****************************
# layer { # subsample ab through pooling
#   name: 'data_ab_ss'
#   type: 'Pooling'
#   bottom: "data_ab"
#   top: "data_ab_ss" # quantized gt colors
#   # param {lr_mult: 0 decay_mult: 0}
#   # param {lr_mult: 0 decay_mult: 0}
#   pooling_param {
#     pool: AVE
#     kernel_size: 4
#     stride: 4
#   }
# }
layer { # subsample ab
  name: 'data_ab_ss'
  type: 'Convolution'
  bottom: "data_ab"
  top: "data_ab_ss" # quantized gt colors
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 4
    group: 2
    weight_filler { type: 'constant' value: 1 }
  }
}
layer { # encode
  type: 'Python'
  name: 'ab_enc'
  bottom: "data_ab_ss"
  top: "gt_ab_313" # quantized gt colors
  python_param {
    module: 'caffe_traininglayers'
    layer: 'NNEncLayer'
  }
}
layer { # compute gray mask
  type: 'Python'
  name: 'nongray_mask'
  bottom: "data_ab_ss"
  top: "nongray_mask" # quantized gt colors
  python_param {
    module: 'caffe_traininglayers'
    layer: 'NonGrayMaskLayer'
  }
}
layer { # compute prior boost
  type: 'Python'
  name: 'prior_boost'
  bottom: "gt_ab_313"
  top: "prior_boost" # quantized gt colors
  python_param {
    module: 'caffe_traininglayers'
    layer: 'PriorBoostLayer'
  }
}
layer { # multiply nongray mask and prior boost
  type: 'Eltwise'
  name: 'prior_boost_nongray'
  bottom: "prior_boost"
  bottom: "nongray_mask"
  top: "prior_boost_nongray"
  eltwise_param {
    operation: 0
  }
}

# *****************
# ***** conv1 *****
# *****************
layer {
  name: "bw_conv1_1"
  type: "Convolution"
  bottom: "data_l"
  top: "conv1_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "conv1_2norm"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv2 *****
# *****************
layer {
  name: "conv2_1"
  type: "Convolution"
  # bottom: "conv1_2"
  bottom: "conv1_2norm"
  # bottom: "pool1"
  top: "conv2_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "conv2_2norm"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv3 *****
# *****************
layer {
  name: "conv3_1"
  type: "Convolution"
  # bottom: "conv2_2"
  bottom: "conv2_2norm"
  # bottom: "pool2"
  top: "conv3_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "conv3_3norm"
  type: "BatchNorm"
  bottom: "conv3_3"
  top: "conv3_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv4 *****
# *****************
layer {
  name: "conv4_1"
  type: "Convolution"
  # bottom: "conv3_3"
  bottom: "conv3_3norm"
  # bottom: "pool3"
  top: "conv4_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "conv4_3norm"
  type: "BatchNorm"
  bottom: "conv4_3"
  top: "conv4_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv5 *****
# *****************
layer {
  name: "conv5_1"
  type: "Convolution"
  # bottom: "conv4_3"
  bottom: "conv4_3norm"
  # bottom: "pool4"
  top: "conv5_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE  
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "conv5_3norm"
  type: "BatchNorm"
  bottom: "conv5_3"
  top: "conv5_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv6 *****
# *****************
layer {
  name: "conv6_1"
  type: "Convolution"
  bottom: "conv5_3norm"
  top: "conv6_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_1"
  type: "ReLU"
  bottom: "conv6_1"
  top: "conv6_1"
}
layer {
  name: "conv6_2"
  type: "Convolution"
  bottom: "conv6_1"
  top: "conv6_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_2"
  type: "ReLU"
  bottom: "conv6_2"
  top: "conv6_2"
}
layer {
  name: "conv6_3"
  type: "Convolution"
  bottom: "conv6_2"
  top: "conv6_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_3"
  type: "ReLU"
  bottom: "conv6_3"
  top: "conv6_3"
}
layer {
  name: "conv6_3norm"
  type: "BatchNorm"
  bottom: "conv6_3"
  top: "conv6_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv7 *****
# *****************
layer {
  name: "conv7_1"
  type: "Convolution"
  bottom: "conv6_3norm"
  top: "conv7_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_1"
  type: "ReLU"
  bottom: "conv7_1"
  top: "conv7_1"
}
layer {
  name: "conv7_2"
  type: "Convolution"
  bottom: "conv7_1"
  top: "conv7_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_2"
  type: "ReLU"
  bottom: "conv7_2"
  top: "conv7_2"
}
layer {
  name: "conv7_3"
  type: "Convolution"
  bottom: "conv7_2"
  top: "conv7_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_3"
  type: "ReLU"
  bottom: "conv7_3"
  top: "conv7_3"
}
layer {
  name: "conv7_3norm"
  type: "BatchNorm"
  bottom: "conv7_3"
  top: "conv7_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv8 *****
# *****************
layer {
  name: "conv8_1"
  type: "Deconvolution"
  bottom: "conv7_3norm"
  top: "conv8_1"
  convolution_param {
    num_output: 256
    kernel_size: 4
    pad: 1
    dilation: 1
    stride: 2
  }
}
layer {
  name: "relu8_1"
  type: "ReLU"
  bottom: "conv8_1"
  top: "conv8_1"
}
layer {
  name: "conv8_2"
  # name: "conv8_2_"
  type: "Convolution"
  bottom: "conv8_1"
  top: "conv8_2"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu8_2"
  type: "ReLU"
  bottom: "conv8_2"
  top: "conv8_2"
}
layer {
  name: "conv8_3"
  type: "Convolution"
  bottom: "conv8_2"
  top: "conv8_3"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu8_3"
  type: "ReLU"
  bottom: "conv8_3"
  top: "conv8_3"
}
# # ****************************
# # ***** Unary prediction *****
# # ****************************
# # layer {
# #   name: "conv5_313"
# #   type: "Convolution"
# #   bottom: "conv5_3"
# #   # bottom: "conv5_3norm"
# #   top: "conv5_313"
# #   convolution_param {
# #     num_output: 313
# #     kernel_size: 1
# #     stride: 1
# #     dilation: 1
# #   }
# # }
# # layer {
# #   name: "conv6_313"
# #   type: "Convolution"
# #   bottom: "conv6_3"
# #   # bottom: "conv6_3norm"
# #   top: "conv6_313"
# #   convolution_param {
# #     num_output: 313
# #     kernel_size: 1
# #     stride: 1
# #     dilation: 1
# #   }
# # }
# layer {
#   name: "conv7_313"
#   type: "Convolution"
#   bottom: "conv7_3"
#   # bottom: "conv7_3norm"
#   top: "conv7_313"
#   convolution_param {
#     num_output: 313
#     kernel_size: 1
#     stride: 1
#     dilation: 1
#   }
# }
# layer {
#   name: "conv8_313"
#   type: "Convolution"
#   bottom: "conv8_3"
#   # bottom: "conv7_3norm"
#   top: "conv8_313"
#   convolution_param {
#     num_output: 313
#     kernel_size: 1
#     stride: 1
#     dilation: 1
#   }
# }
# # *******************
# # ***** Softmax *****
# # *******************
# # layer {
# #   name: "Softmax5"
# #   type: "Softmax"
# #   bottom: "conv5_313"
# #   top: "class5_313"
# # }
# # layer {
# #   name: "Softmax6"
# #   type: "Softmax"
# #   bottom: "conv6_313"
# #   top: "class6_313"
# # }
# layer {
#   name: "Softmax7"
#   type: "Softmax"
#   bottom: "conv7_313"
#   top: "class7_313"
# }
# layer {
#   name: "Softmax8"
#   type: "Softmax"
#   bottom: "conv8_313"
#   top: "class8_313"
# }
# layer {
#   name: "Silence"
#   type: "Silence"
#   # bottom: "class5_313"
#   # bottom: "class6_313"
#   bottom: "class7_313"
#   bottom: "class8_313"
# }
# ****************************
# ***** Unary prediction *****
# ****************************
# layer {
#   name: "conv5_313"
#   type: "Convolution"
#   bottom: "conv5_3"
#   # bottom: "conv5_3norm"
#   top: "conv5_313"
#   convolution_param {
#     num_output: 313
#     kernel_size: 1
#     stride: 1
#     dilation: 1
#   }
# }
# layer {
#   name: "conv6_313"
#   type: "Convolution"
#   bottom: "conv6_3"
#   # bottom: "conv6_3norm"
#   top: "conv6_313"
#   convolution_param {
#     num_output: 313
#     kernel_size: 1
#     stride: 1
#     dilation: 1
#   }
# }
# layer {
#   name: "conv7_313"
#   type: "Convolution"
#   bottom: "conv7_3"
#   # bottom: "conv7_3norm"
#   top: "conv7_313"
#   convolution_param {
#     num_output: 313
#     kernel_size: 1
#     stride: 1
#     dilation: 1
#   }
# }
layer {
  name: "conv8_313"
  type: "Convolution"
  bottom: "conv8_3"
  # bottom: "conv7_3norm"
  top: "conv8_313"
  convolution_param {
    num_output: 313
    kernel_size: 1
    stride: 1
    dilation: 1
  }
}
# *************************************
# ***** Upsampling classification *****
# *************************************
# layer {
#   name: "conv5_us"
#   type: "Deconvolution"
#   bottom: "conv5_313"
#   top: "conv5_313_us"
#   param {name: "us_w" lr_mult: 0 decay_mult: 0}
#   param {name: "us_b" lr_mult: 0 decay_mult: 0}
#   convolution_param {
#     num_output: 313
#     group: 313
#     kernel_size: 4
#     pad: 1
#     stride: 2
#   }
# }
# layer {
#   name: "conv6_us"
#   type: "Deconvolution"
#   bottom: "conv6_313"
#   top: "conv6_313_us"
#   param {name: "us_w" lr_mult: 0 decay_mult: 0}
#   param {name: "us_b" lr_mult: 0 decay_mult: 0}
#   convolution_param {
#     num_output: 313
#     group: 313
#     kernel_size: 4
#     pad: 1
#     stride: 2
#   }
# }
# layer {
#   name: "conv7_us"
#   type: "Deconvolution"
#   bottom: "conv7_313"
#   top: "conv7_313_us"
#   param {name: "us_w" lr_mult: 0 decay_mult: 0}
#   param {name: "us_b" lr_mult: 0 decay_mult: 0}
#   convolution_param {
#     num_output: 313
#     group: 313
#     kernel_size: 4
#     pad: 1
#     stride: 2
#   }
# }
# ***************************
# ***** Boosting priors *****
# ***************************
# layer {
#   name: "PriorBoost5"
#   type: "Python"
#   bottom: "conv5_313_us"
#   bottom: "prior_boost"
#   top: "conv5_313_boost"
#   python_param {
#     module: 'caffe_traininglayers'
#     layer: 'ClassRebalanceMultLayer'
#   }
# }
# layer {
#   name: "PriorBoost6"
#   type: "Python"
#   bottom: "conv6_313_us"
#   # bottom: "prior_boost"
#   bottom: "prior_boost_nongray"
#   top: "conv6_313_boost"
#   python_param {
#     module: 'caffe_traininglayers'
#     layer: 'ClassRebalanceMultLayer'
#   }
# }
# layer {
#   name: "PriorBoost7"
#   type: "Python"
#   bottom: "conv7_313_us"
#   # bottom: "prior_boost"
#   bottom: "prior_boost_nongray"
#   top: "conv7_313_boost"
#   python_param {
#     module: 'caffe_traininglayers'
#     layer: 'ClassRebalanceMultLayer'
#   }
# }
layer {
  name: "conv8_313_boost"
  type: "Python"
  bottom: "conv8_313"
  # bottom: "prior_boost"
  bottom: "prior_boost_nongray"
  top: "conv8_313_boost"
  python_param {
    module: 'caffe_traininglayers'
    layer: 'ClassRebalanceMultLayer'
  }
}
# ************************
# ***** Softmax loss *****
# ************************
# layer {
#   name: "SoftmaxLoss5"
#   type: "SoftmaxCrossEntropyLoss"
#   bottom: "conv5_313_boost"
#   bottom: "gt_ab_313"
#   top: 'loss5_313'
#   loss_weight: 0.125
# }
# layer {
#   name: "SoftmaxLoss6"
#   type: "SoftmaxCrossEntropyLoss"
#   bottom: "conv6_313_boost"
#   bottom: "gt_ab_313"
#   top: 'loss6_313'
#   loss_weight: 0.25
# }
# layer {
#   name: "SoftmaxLoss7"
#   type: "SoftmaxCrossEntropyLoss"
#   bottom: "conv7_313_boost"
#   bottom: "gt_ab_313"
#   top: 'loss7_313'
#   loss_weight: 0.5
# }
layer {
  name: "loss8_313"
  type: "SoftmaxCrossEntropyLoss"
  bottom: "conv8_313_boost"
  bottom: "gt_ab_313"
  top: 'loss8_313'
  loss_weight: 1.0
}
# *******************
# ***** Softmax *****
# *******************
# layer {
#   name: "Softmax5"
#   type: "Softmax"
#   bottom: "conv5_313"
#   top: "class5_313"
# }
# layer {
#   name: "Softmax6"
#   type: "Softmax"
#   bottom: "conv6_313"
#   top: "class6_313"
# }
# layer {
#   name: "Softmax7"
#   type: "Softmax"
#   bottom: "conv7_313"
#   top: "class7_313"
# }
# layer {
#   name: "Softmax8"
#   type: "Softmax"
#   bottom: "conv8_313"
#   top: "class8_313"
# }
